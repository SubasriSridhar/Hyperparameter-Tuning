### Hyperparameter tuning 

Randomized search is used for parameter optimization which results in the highest 5-fold cross-validated accuracy score. The best performing models will be used for the Voting classifier model. The area under the ROC curve (AUC-ROC) and the area under the precision and recall curve are main estimators to decide the best algorithm.

Hyperparameter tuning is the best way to determine the optimal settings. It tests the output of each configuration in many different combinations. This depends more on the output than on the hypothesis. Optimizing the model on training data is not a right solution, as the model is not generalized. It might perform poorly on a test data set which leads to the overfitting problem. In order to avoid the model memorizing the training data, hyperparameter optimization cross validates the model many times using folds. This way the training data is set very well and can apply to real time results. (Mithrakumar, 2019) and (Jordan, 2017)

One of the most common cross validation methods is K-Fold CV. In any machine learning model, the first step is to split the data into a train and test set. When trying hyperparameter tuning on the data using K-Fold CV where k is the folds (number of subsets from the data), say, the model is trained with cross-validation k value as 5. It is an iterative approach, on the first iteration it trains on 4 folds and validates the 5th fold. On the next iteration it trains 1,2,3 and 5th fold leaving 4th for validation. (Koehrsen, 2018)The same procedure is repeated for three more times by validating different fold every time. By the end of the model, the end results with the best parameter is selected. This iteration is a very long process. Suppose there are 10 hyperparameter tuning parameters to be tested along 5 cross fold validations, then the algorithm should repeat itself 50 times with different test and training set everytime. However, the effort for this process is made simpler with the help Scikit-Learn model. In general, there will be a fuzzy idea of the best hyperparameters and therefore the right approach to narrowing the scope is to evaluate a full range of values for each hyperparameter. Using the RandomizedSearchCV approach of Scikit-Learn, a grid of hyperparameter ranges is described, and randomly sampled from the array, running K-Fold CV with every combination of values. (Lan, 2017)

All the algorithms in the project uses hyper parameter model tuning, all of them are briefly explained in the later section wherever it is used. Few important parameters in RandomizedSearchCV are n_iter which is used to define the number of combinations that the algorithm should try, and cv is the k-number of folds. Increasing them will increase the run time, so the developer has to decide on the trade-offs between performance and time. (scikit-learn, 2019)
